{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR TESTING PURPOSES, MAY CONTAIN ERRORS/OUTDATED CODE\n",
    "\n",
    "import torch\n",
    "from dataset import PippaDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load the model without downloading (assuming you have it locally)\n",
    "model_name = 'openai-community/gpt2'  # Replace with your actual model name\n",
    "# model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto' if device == \"cuda\" else 'cpu',\n",
    "    quantization_config=quant_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters() * 4 / 8 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134060568"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = \"data/pippa_deduped.jsonl\"\n",
    "dataset = PippaDataset(dataset_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_inputs = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='left',\n",
    ")\n",
    "\n",
    "tokenizer_outputs = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='right',\n",
    ")\n",
    "\n",
    "if tokenizer_inputs.pad_token is None:\n",
    "    tokenizer_inputs.pad_token = tokenizer_inputs.eos_token  # Ensure pad token is set\n",
    "    tokenizer_outputs.pad_token = tokenizer_outputs.eos_token  # Ensure pad token is set\n",
    "    tokenizer_inputs.pad_token_id = tokenizer_inputs.eos_token_id  # Ensure pad token is set\n",
    "    tokenizer_outputs.pad_token_id = tokenizer_outputs.eos_token_id  # Ensure pad token is set\n",
    "\n",
    "dataset.set_chat_template_params('prompt_templates/vicuna_prompt_template.jinja', tokenizer_inputs)\n",
    "# Prepare sample data\n",
    "sample_size = 2  # Adjust as needed\n",
    "sampled_data = dataset.sample_dataset(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts = ['The capital of France', 'The capital of Germany is']\n",
    "# target_texts = [' is Paris', ' Berlin']\n",
    "# unzip the sampled data\n",
    "sampled_data = [\n",
    "    (\"What is the capital of France?\", \"\\n\\nParis is the capital of France.\")\n",
    "]\n",
    "contexts, target_texts = zip(*sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = model.config.max_position_embeddings\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer_outputs(\n",
    "    target_texts, \n",
    "    return_tensors='pt', \n",
    "    padding=True,\n",
    "    add_special_tokens=False,\n",
    ") # this will put padding to the right and add only the eos token\n",
    "\n",
    "# get the max length of the input by subtracting the length of the targets from the max length\n",
    "max_input_len = max_len - outputs['input_ids'].shape[1]\n",
    "\n",
    "inputs = tokenizer_inputs(\n",
    "    contexts, \n",
    "    return_tensors='pt', \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=max_input_len,\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "\n",
    "# concatenate the inputs and targets and their attention masks\n",
    "input_ids = torch.cat([inputs['input_ids'], outputs['input_ids']], dim=1).to(device)\n",
    "attention_mask = torch.cat([inputs['attention_mask'], outputs['attention_mask']], dim=1).to(device)\n",
    "\n",
    "# get the mask that only give us the output ids\n",
    "output_ids_mask = torch.cat(\n",
    "    [\n",
    "        torch.zeros_like(inputs['attention_mask']), \n",
    "        outputs['attention_mask']\n",
    "    ], dim=1\n",
    ")\n",
    "\n",
    "# shift the mask to the right by one\n",
    "output_ids_mask = torch.cat(\n",
    "    [\n",
    "        torch.zeros_like(output_ids_mask[:, :1]), \n",
    "        output_ids_mask[:, :-1]\n",
    "    ], dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift the output.logits to the right by one\n",
    "shifted_logits = torch.cat(\n",
    "    [\n",
    "        torch.zeros_like(outputs.logits[:, :1, :]), \n",
    "        outputs.logits[:, :-1, :]\n",
    "    ], dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do softmax to get the probabilities\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3063, 0.2089, 0.2340, 0.0815, 0.8290, 0.0931, 0.8701, 0.2490, 0.2989,\n",
       "         0.5338, 0.1337, 0.9981, 0.3948, 0.6096, 0.8346, 0.8078, 0.9467, 0.5783,\n",
       "         0.3961]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.max(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of France? \n",
      "\n",
      "Paris is the capital of France.\n",
      "#######################\n",
      "# is the difference of the?\n",
      " Paris\n",
      "Paris. the capital of France.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decode the the argmax of the probabilities\n",
    "token_text = tokenizer_inputs.batch_decode(input_ids)\n",
    "\n",
    "# print the first example, token_text is the input, token_output_text is the output\n",
    "print(token_text[0])\n",
    "print('#######################')\n",
    "# get the argmax of the probabilities and decode it\n",
    "output_ids = probabilities.argmax(dim=-1)\n",
    "output_text = tokenizer_outputs.batch_decode(output_ids)\n",
    "print(output_text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities of input_ids by indexing the probabilities tensor\n",
    "token_probabilities = probabilities.gather(-1, input_ids.unsqueeze(-1)).squeeze(-1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_probabilities = token_probabilities * output_ids_mask.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0536, device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_probabilities.sum() / output_ids_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,    13,\n",
       "          3916,   278, 28723,   272,  5565,   302,  4843, 28723,    13]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids * output_ids_mask.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> -> #\n",
      "What -> is\n",
      "is -> the\n",
      "the -> difference\n",
      "capital -> of\n",
      "of -> the\n",
      "France -> ?\n",
      "? -> \n",
      "\n",
      " -> Paris\n",
      "\n",
      " -> \n",
      "\n",
      "\n",
      " -> Par\n",
      "Par -> is\n",
      "is -> .\n",
      "is -> the\n",
      "the -> capital\n",
      "capital -> of\n",
      "of -> France\n",
      "France -> .\n",
      ". -> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output_ids[0])):\n",
    "    print(tokenizer_outputs.decode(input_ids[0][i]), '->', tokenizer_outputs.decode(output_ids[0][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn(1, 2, 101)\n",
    "\n",
    "logits = torch.cat(\n",
    "    [\n",
    "        torch.zeros_like(logits[:, :1, :]), \n",
    "        logits[:, :-1, :]\n",
    "    ], dim=1\n",
    ")\n",
    "\n",
    "if torch.isnan(outputs.logits).any():\n",
    "    raise ValueError(\"NaN values detected llm -> outputs.logits tensor\")\n",
    "\n",
    "# Only keep the top PROB_TOP_K scores by -inf the rest\n",
    "# This will make the model only consider the top 100 tokens and make sure the models with higher vocab sizes are not penalized\n",
    "\n",
    "# # get the top k logits and create a mask for them\n",
    "top_k_logits, top_k_indices = logits.topk(100, dim=-1)\n",
    "mask = torch.full_like(logits, float('-inf')).scatter(-1, top_k_indices, top_k_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
